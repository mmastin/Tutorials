{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALE = 'male'\n",
    "FEMALE = 'female'\n",
    "UNKNOWN = 'unknown'\n",
    "BOTH = 'both'\n",
    "\n",
    "MALE_WORDS = set([\n",
    "        'guy','spokesman','chairman',\"men's\",'men','him',\"he's\",'his',\n",
    "        'boy','boyfriend','boyfriends','boys','brother','brothers','dad',\n",
    "        'dads','dude','father','fathers','fiance','gentleman','gentlemen',\n",
    "        'god','grandfather','grandpa','grandson','groom','he','himself',\n",
    "        'husband','husbands','king','male','man','mr','nephew','nephews',\n",
    "        'priest','prince','son','sons','uncle','uncles','waiter','widower',\n",
    "        'widowers'\n",
    "])\n",
    "\n",
    "FEMALE_WORDS = set([\n",
    "        'heroine','spokeswoman','chairwoman',\"women's\",'actress','women',\n",
    "        \"she's\",'her','aunt','aunts','bride','daughter','daughters','female',\n",
    "        'fiancee','girl','girlfriend','girlfriends','girls','goddess',\n",
    "        'granddaughter','grandma','grandmother','herself','ladies','lady',\n",
    "        'lady','mom','moms','mother','mothers','mrs','ms','niece','nieces',\n",
    "        'priestess','princess','queens','she','sister','sisters','waitress',\n",
    "        'widow','widows','wife','wives','woman'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genderize(words):\n",
    "    \n",
    "    mwlen = len(MALE_WORDS.intersection(words))\n",
    "    fwlen = len(FEMALE_WORDS.intersection(words))\n",
    "    \n",
    "    if mwlen > 0 and fwlen == 0:\n",
    "        return MALE\n",
    "    elif mwlen == 0 and fwlen > 0:\n",
    "        return FEMALE\n",
    "    elif mwlen > 0 and fwlen > 0:\n",
    "        return BOTH\n",
    "    else:\n",
    "        return UNKNOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_gender(sentences):\n",
    "    \n",
    "    sents = Counter()\n",
    "    words = Counter()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        gender = genderize(sentence)\n",
    "        sents[gender] += 1\n",
    "        words[gender] += len(sentence)\n",
    "        \n",
    "    return sents, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def parse_gender(text):\n",
    "    \n",
    "    sentences = [\n",
    "        [word.lower() for word in nltk.word_tokenize(sentence)]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    \n",
    "    sents, words = count_gender(sentences)\n",
    "    total = sum(words.values())\n",
    "    \n",
    "    for gender, count in words.items():\n",
    "        pcent = (count / total) * 100\n",
    "        nsents = sents[gender]\n",
    "        \n",
    "        print(\n",
    "        '{}% {} ({} sentences)'.format(pcent, gender, nsents)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New York Times entitled “Rehearse, Ice Feet, Repeat: The Life of a New York City Ballet Corps Dancer”\n",
    "\n",
    "article = (\"With apologies to James Brown, the hardest working people in show business may well be ballet dancers. And at New York City Ballet, none work harder than the dancers in its lowest rank, the corps de ballet. During the first week of the company’s winter season, Claire Kretzschmar, 24, a rising corps member, danced in all seven performances, appearing in five ballets, sometimes changing costumes at intermission to dance two roles in a night.     But her work onstage did not even begin to capture the stamina required to be in the corps. Spending a week shadowing Ms. Kretzschmar was exhausting — she gave new meaning to the idea of being on your feet all day. Twelve-hour days at the David H. Koch Theater, the company’s Lincoln Center home, were hardly unusual: Company class each morning was followed by back-to-back-to-back rehearsals, with occasional breaks for costume fittings or physical therapy, and then by the hair-makeup-costume-dance routine of daily performances. This weekend will be even more frenetic. Ms. Kretzschmar will appear in seven ballets from Friday evening to Sunday afternoon, when she faces a new test: taking on the title role of the Sleepwalker in George Balanchine’s eerie, proto-goth ballet “La Sonnambula.” Balanchine, one of ballet’s most important choreographers, was a founder of City Ballet, and remains its guiding spirit more than three decades after his death. Being in City Ballet’s corps is not like being a member of a chorus line, or a backup singer. The company promotes almost all of its stars, the principal dancers, from within. So while the corps is expected to be able to move in startling unison, like a school of fish, and to assemble in straight lines and keep all rippling swan arms parallel, its 54 members are also competing for bigger roles and promotions. “There is an element of competition, and people get different opportunities, but everybody just wants to do their best onstage, and everyone wants each other to just do their best onstage,” Ms. Kretzschmar said during a break between rehearsals last week. “We have all experienced so many extreme highs and lows that it’s almost that you have to bond with this group of people. Here are scenes from one week in the busy life of a corps member.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.57630979498862% unknown (11 sentences)\n",
      "9.79498861047836% female (2 sentences)\n",
      "16.62870159453303% both (1 sentences)\n"
     ]
    }
   ],
   "source": [
    "parse_gender(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
    "\n",
    "DOC_PATTERN = r'(?!\\.)[\\w)\\s]+/[\\w\\s\\d\\-]+\\.txt'\n",
    "CAT_PATTERN = r'([\\w)\\s]+)/.*'\n",
    "\n",
    "# corpus = CategorizedPlaintextCorpusReader(\n",
    "#     '/path/to/corpus/root', DOC_PATTERN, cat_pattern=CAT_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "import codecs\n",
    "\n",
    "CAT_PATTERN =  r'([\\w)\\s]+)/.*'\n",
    "DOC_PATTERN = r'(?!\\.)[\\w)\\s]+/[\\w\\s\\d\\-]+\\.txt'\n",
    "\n",
    "TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\n",
    "\n",
    "class HTMLCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    # corpus reader for raw html docs to enable preprocessing\n",
    "    \n",
    "    def __init__(self, root, fileids=DOC_PATTERN, encoding='utf8',\n",
    "                tags=TAGS, **kwargs):\n",
    "        \n",
    "        # add default category pattern if not passed into the class\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "            \n",
    "        # initialize NLTK corpus reader objects\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "        \n",
    "        # save tags specifically want to extract\n",
    "        self.tags = tags\n",
    "        \n",
    "    \n",
    "    def resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        returns list of fields or categories depending on what is passed\n",
    "        to each internal corpus reader function. implemented similarly\n",
    "        to NLTK CategorizedPlaintextCorpusReader\n",
    "        \"\"\"\n",
    "        \n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError('specify fileids or categories, not both')\n",
    "            \n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "    \n",
    "    \n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        returns complete text of HTML doc, closing doc after done\n",
    "        reading it and yielding in memory safe fashion\n",
    "        \"\"\"\n",
    "        \n",
    "        # resolve fileids and categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        \n",
    "        # create generator, loading one doc into memory at a time\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                yield f.read()\n",
    "                \n",
    "                \n",
    "    def sizes(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        returns list of tuples, fileid and size on disk of file.\n",
    "        this function is used to detect oddly large files in corpus\n",
    "        \"\"\"\n",
    "        \n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        \n",
    "        for path in self.abspaths(fileids):\n",
    "            yield os.path.getsize(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "class SqliteCorpusReader(object):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self._cur = sqlite3.connect(path).cursor()\n",
    "        \n",
    "    def ids(self):\n",
    "        \"\"\"\n",
    "        returns review ids, which enable joins to other metadata\n",
    "        \"\"\"\n",
    "        self._cur.execute('SELECT reviewid FROM content')\n",
    "        for idx in iter(self._cur.fetchone, None):\n",
    "            yield idx\n",
    "            \n",
    "    def scores(self):\n",
    "        \"\"\"\n",
    "        returns review score to be used as the target for\n",
    "        later supervised learning problems\n",
    "        \"\"\"\n",
    "        self._cur.execute('SELECT score FROM reviews')\n",
    "        for score in iter(self._cur.fetchone, None):\n",
    "            yield score\n",
    "            \n",
    "    def texts(self):\n",
    "        \"\"\"\n",
    "        returns full review texts, to be preprocessed and\n",
    "        vectorized for supervised learning\n",
    "        \"\"\"\n",
    "        self._cur.execute('SELECT content FROM content')\n",
    "        for text in iter(self._cur.fetchone, None):\n",
    "            yield text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readability.readability import Unparseable\n",
    "from readability.readability import Document as Paper\n",
    "\n",
    "def html(self, fileids=None, categories=None):\n",
    "    \"\"\"\n",
    "    returns HTML content of eachd ocument, cleaning it using\n",
    "    the readability-lxml library\n",
    "    \"\"\"\n",
    "    for doc in self.docs(fileids, categories):\n",
    "        try:\n",
    "            yield Paper(doc).summary()\n",
    "        except Unparseable as e:\n",
    "            print('Could not parse HTML: {}'.format(e))\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\n",
    "\n",
    "def paras(self, fileids=None, categories=None):\n",
    "    \"\"\"\n",
    "    uses BeautifulSoup to parse paragraphs from HTML.\n",
    "    \"\"\"\n",
    "    for html in self.html(fileids, categories):\n",
    "        soup = bs4.BeautifulSoup(html, 'lxml')\n",
    "        for element in soup.find_all(tags):\n",
    "            yield element.text\n",
    "        # destroys tree when done with each file\n",
    "        soup.decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import pos_tag, sent_tokenize\n",
    "\n",
    "def sents(self, fileids=None, categories=None):\n",
    "    \"\"\"\n",
    "    use built in sentence tokenizer to extract sentences. this method uses\n",
    "    BS to parse HTML\n",
    "    \"\"\"\n",
    "    for paragraph in self.paras(fileids, categories):\n",
    "        for sentence in sent_tokenize(paragraph):\n",
    "            yield sentence\n",
    "            \n",
    "def words(self, fileids=None, categories=None):\n",
    "    for sentence in self.sents(fileids, categories):\n",
    "        for token in wordpunct_tokenize(sentence):\n",
    "            yield token\n",
    "            \n",
    "def tokenize(self, fileids=None, categories=None):\n",
    "    \"\"\"\n",
    "    segments, tokenizes and tags a document in corpus\n",
    "    \"\"\"\n",
    "    for paragraph in self.paras(fileids=fileids):\n",
    "        yield [\n",
    "            pos_tag(wordpunct_tokenize(sent))\n",
    "            for sent in sent_tokenize(paragraph)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def describe(self, fileids=None, categories=None):\n",
    "    \"\"\"\n",
    "    performs single pass of corpus and returns dict with variety\n",
    "    of metrics concerning state of corpus\n",
    "    \"\"\"\n",
    "    started = time.time()\n",
    "    \n",
    "    # structure to perform counting\n",
    "    counts = nltk.FreqDist()\n",
    "    tokens = nltk.FreqDist()\n",
    "    \n",
    "    # perform single pass over paragraphs, tokenize and count\n",
    "    for para in self.paras(fileids, categories):\n",
    "        counts['paras'] += 1\n",
    "        \n",
    "        for sent in para:\n",
    "            counts['sents'] += 1\n",
    "            \n",
    "            for word, tag in sent:\n",
    "                counts['words'] += 1\n",
    "                tokens[word] += 1\n",
    "                \n",
    "    # computer number of files and categories in corpus\n",
    "    n_fileids = len(self.resolve(fileids, categories) or self.fileids())\n",
    "    n_topics = len(self.categories(self.resolve(fileids, categories)))\n",
    "    \n",
    "    # return data structure with info\n",
    "    return {\n",
    "        'files': n_fileids,\n",
    "        'topics': n_topics,\n",
    "        'paras': counts['paras'],\n",
    "        'sents': counts['sents'],\n",
    "        'words': counts['words'],\n",
    "        'vocab': len(tokens),\n",
    "        'lexdiv': float(counts['words']) / float(len(tokens)), \n",
    "        'ppdoc': float(counts['paras']) / float(n_fileids), \n",
    "        'sppar': float(counts['sents']) / float(counts['paras']), \n",
    "        'secs': time.time()-started,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "import pickle\n",
    "\n",
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    preprocessor wraps an 'HTMLCorpusReader' and performs tokenization\n",
    "    and part-of-speech tagging\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus, target=None, **kwargs):\n",
    "        self.corpus = corpus\n",
    "        self.target = target\n",
    "        \n",
    "    def fileids(self, fileids=None, categories=None):\n",
    "        fileids = self.corpus.resolve(fileids, categories)\n",
    "        if fileids:\n",
    "            return fileids\n",
    "        return self.corpus.fileids()\n",
    "    \n",
    "    def abspath(self, fileid):\n",
    "        # find directory, relative to corpus root\n",
    "        parent = os.path.relpath(\n",
    "            os.path.dirname(self.corpus.abspath(fileid)), self.corpus.root)\n",
    "        \n",
    "        # computer the name parts to reconstruct\n",
    "        basename = os.path.basename(fileid)\n",
    "        name, ext = os.path.splitext(basename)\n",
    "        \n",
    "        # create pickle file extension\n",
    "        basename = name + '.pickle'\n",
    "        \n",
    "        # return path to file relative to the target\n",
    "        return os.path.normpath(os.path.join(self.target, parent, basename))\n",
    "    \n",
    "    def tokenize(self, fileid):\n",
    "        for paragraph in self.corpus.paras(fileids=fileid):\n",
    "            yield [\n",
    "                pos_tag(wordpunct_tokenize(sent))\n",
    "                for sent in sent_tokenize(paragraph)\n",
    "            ]\n",
    "            \n",
    "    def process(self, fileid):\n",
    "        \"\"\"\n",
    "        for a single file, checks location on disk to ensure no errors,\n",
    "        uses +tokenize()+ to perform preprocessing and writes transformed\n",
    "        doc as a pickle to target location\n",
    "        \"\"\"\n",
    "        # compute outpath to write the file to\n",
    "        target = self.abspath(fileid)\n",
    "        parent = os.path.dirname(target)\n",
    "        \n",
    "        # make sure directory exists\n",
    "        if not os.path.exists(parent):\n",
    "            os.makedirs(parent)\n",
    "            \n",
    "        # make sure that the parent is a directory and not a file\n",
    "        if not os.path.isdir(parent):\n",
    "            raise ValueError('Please supply a directory to write preprocessed data to.')\n",
    "            \n",
    "        # create a data structure for the pickle\n",
    "        document = list(self.tokenize(fileid))\n",
    "        \n",
    "        # open and serialize the pickle to disk\n",
    "        with open(target, 'wb') as f:\n",
    "            pickle.dmp(document, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        # clean up the document\n",
    "        del document\n",
    "        \n",
    "        return target\n",
    "    \n",
    "    def transform(self, fileids=None, categories=None):\n",
    "        # make target directory if it doesn't already exist\n",
    "        if not os.path.exists(self.target):\n",
    "            os.makedirs(self.target)\n",
    "            \n",
    "        # resolve fileids to start processing\n",
    "        for fileid in self.fileids(fileids, categories):\n",
    "            yield self.process(fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "\n",
    "class PickledCorpusReader(HTMLCorpusReader):\n",
    "    \n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "        \n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        # load one pickled document into memory at a time\n",
    "        for path in self.abspaths(fileids):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "                \n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for para in doc:\n",
    "                yield para\n",
    "                \n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        for para in self.paras(fileids, categories):\n",
    "            for sent in para:\n",
    "                yield sent\n",
    "                \n",
    "    def tagged(self, fileids=None, categories=None):\n",
    "        for sent in self.sents(fileids, categories):\n",
    "            for tagged_token in sent:\n",
    "                yield tagged_token\n",
    "                \n",
    "    def words(self, fileids=None, categories=None):\n",
    "        for tagged in self.tagged(fileids, categories):\n",
    "            yield tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "    \n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctation: continue\n",
    "        yield stem.stem(token)\n",
    "        \n",
    "corpus = [\n",
    "    \"The elephant sneezed at the sight of potatoes.\",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
